---
title: "acquisition of TEs"
date: "Last update: Nov 27th 2020"
output: 
  html_document: 
    code_folding: show
    collapsed: no
    df_print: kable
    highlight: espresso
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_float: yes
editor_options: 
  chunk_output_type: console
---
# Preparations
## load libraries
```{r}

# install.packages("devtools")
#devtools::install_github("mikabr/ggpirate")

library(here)
library(tidyverse)
library(tidylog)
library(wordbankr)
library(ggplot2)
library(patchwork)
library(gridExtra)
library("ggpubr")

library(lme4)
library(lmerTest)
library(scales)

`%notin%` <- Negate(`%in%`)

```

# load data
```{r}
cdi_ws = readRDS(here::here("data_raw","output_analyzeSent.Rdata"))
```

# Participant Exclusions
exclude babies out of 18-33m age range, monolinguals and those with >10% 3rd language exposure
```{r}
# exclude kids that are bilingual at one visit, monolingual at another (regardless of direction)
exclude.lang_group_change <- cdi_ws %>%
  filter(lang_group !="other") %>%
  group_by(baby_id) %>% 
  distinct(ID_testdate, .keep_all = T) %>%
  mutate(visit_num = 1:n()) %>%  
  mutate(changed_language_group_status = if_else(visit_num == 1, NA, 
                                                 lang_group != lag(lang_group))) %>%
  # uncomment see all visits for babies that have one visit excluded
  #filter(any(changed_language_group_status == TRUE) ) %>% View()
  filter(changed_language_group_status == TRUE) %>% 
  pull(baby_id)


# Create exclusion criteria variables
cdi_ws_TE <- cdi_ws %>%
  mutate(
    # create an exclusion criteria for language
    exclude.language = case_when(
      # exclude babies with more than 10% exposure to a 3rd language
      lang_exp_other > 10 ~ 1,
      # exclude language group status changed from bilingual to mono or vice versa
      baby_id %in% exclude.lang_group_change ~ 1, 
      lang_group == "bilingual" ~ 0, #bilingual
      lang_group == "monolingual" ~ 0, # monolingual
      TRUE ~ 1), # all other cases exlude
    
    # create an exclusion criteria for age
    exclude.age = case_when(!between(age_months_binned, 18, 33) ~ 1, 
                            TRUE ~ 0) # all other cases keep
    )


# Exclude participants
keepers_ws_TE <- cdi_ws_TE %>%
  filter(
    # exclude participants who have exposure to a third language/ are neither bilingual nor monolingual
    exclude.language == 0,
    # exclude participants who don't fit age range
    exclude.age == 0,
    # exclude participants who don't have both CDI filled
    both_cdi_filled == "Y",
    # keep only babies with at least 25% of E and 25% of F (i.e., exclude monolinguals)
    lang_exp_eng >= 25 & lang_exp_eng <= 75,
    lang_exp_fre >= 25 & lang_exp_fre <= 75)


### Check how many distinct participants and data point we have
keepers_ws_TE %>% summarize(N_visits = n_distinct(ID_testdate),
                            N_babies = n_distinct(baby_id),
                            N_admin = sum(n_administrations, na.rm=T))


### Check if there is any NAs in the vocabulary measures (should all return 0)
keepers_ws_TE %>% filter(is.na(vocab_score)) %>% 
  select(ID_testdate, vocab_type, vocab_score) %>%
  nrow()

keepers_ws_TE %>% filter(is.na(eng_unique_words)) %>% 
  select(ID_testdate) %>% 
  nrow()

keepers_ws_TE %>% filter(is.na(fre_unique_words)) %>% 
  select(ID_testdate) %>%
  nrow()

keepers_ws_TE %>% filter(is.na(number_of_te)) %>% 
  select(ID_testdate) %>%
  nrow()
```

# Prepare the dataset
```{r}
keepers_ws_TE <- keepers_ws_TE %>%
  # keep only necessary variables
  select(-c(vocab_type_dom, vocab_cross_lang, n_administrations)) %>% #n_administrations
  spread(vocab_type, vocab_score) %>%
  # calculate age in months (with decimal)
  mutate(age_month_decimal = age_days/30) %>%
  #defining dominance based on input
  #mutate(TotalNW_Dom_input = case_when(lang_dom == "English" ~ total_words_eng,
  #                                        lang_dom == "French" ~ total_words_fre),
  #       TotalNW_NonDom_input = case_when(lang_dom == "English" ~ total_words_fre,
  #                                           lang_dom == "French" ~ total_words_eng),
  #       Dom_unique_words_input = case_when(lang_dom == "French" ~ fre_unique_words, 
  #                                             lang_dom == "English" ~ eng_unique_words),
  #       NonDom_unique_words_input = case_when(lang_dom == "French" ~ eng_unique_words, 
  #                                       lang_dom == "English" ~ fre_unique_words)) %>%
  #mutate(balance_input = balance/100) %>%
  #defining dominance based on vocabulary size
  mutate(LangDom_vocab = case_when(total_words_eng > total_words_fre ~ "English",
                                 TRUE ~ "French")) %>%
  mutate(LangDom_vocab = as.factor(LangDom_vocab)) %>%
  mutate(TotalNW_Dom = case_when(total_words_eng > total_words_fre ~ total_words_eng,
                                 TRUE ~ total_words_fre),
         TotalNW_NonDom = case_when(total_words_eng > total_words_fre ~ total_words_fre,
                                    TRUE ~ total_words_eng),
         Dom_unique_words = case_when(total_words_eng > total_words_fre ~ eng_unique_words,
                                      TRUE ~ fre_unique_words),
         NonDom_unique_words = case_when(total_words_eng > total_words_fre ~ fre_unique_words,
                                         TRUE ~ eng_unique_words)) %>%
  mutate(TotalUnique = Dom_unique_words + NonDom_unique_words) %>%
  #Defining vocabulary-based balance score
  mutate(balance_vocab = TotalNW_NonDom/(TotalNW_Dom+TotalNW_NonDom)) %>%
  #defining dominance based on input (with reference to the lang_dom defined by vocab size)
  mutate(LangDom_input = case_when(LangDom_vocab == "English" ~ lang_exp_eng,
                                    TRUE ~ lang_exp_fre),
         LangNonDom_input = case_when(LangDom_vocab == "English" ~ lang_exp_fre,
                                    TRUE ~ lang_exp_eng)) %>%
  mutate(balance_input = LangNonDom_input/(LangDom_input+LangNonDom_input))

write.csv(keepers_ws_TE, "Data/keepers_ws_TE.csv", row.names = F)

```


# descriptives
## how many distinct participants and data point we have
```{r}
keepers_ws_TE %>% summarize(N_babies = n_distinct(baby_id),
                            N_admin = n_distinct(ID_testdate))

# number of participants who contributed multiple data points
keepers_ws_TE %>% 
  count(baby_id) %>% 
  filter(n>1) %>%
  nrow()

N_duplicatedBabyID <- keepers_ws_TE %>%
  filter(duplicated(baby_id)) # checking which baby_id has multiple data
```

## percentage of english-dominant & french-dominant children
```{r}
# dominance defined by language exposure
keepers_ws_TE %>% group_by(lang_dom) %>% distinct(baby_id) %>%
  count()

# dominance defined by vocab size
keepers_ws_TE %>% 
  distinct(ID_testdate, .keep_all = TRUE) %>%
  mutate(Eng_Dom = ifelse(LangDom_vocab == "English", 1, 0)) %>%
  summarise(n = n(),
            n_EngDom = sum(Eng_Dom),
            percent_EngDom = n_EngDom/n * 100,
            percent_FrDom = 100 - percent_EngDom)
```

## gender: percentage of girls
```{r}
keepers_ws_TE %>%
  group_by(gender) %>% 
  distinct(baby_id, .keep.all=T) %>%
  count() 
```

## demographics table: age (in months), maternal edu in years, lang_exp_eng, lang_exp_fre, lang_exp_other
```{r}
# Create dataframe of demographics
d_participants <- keepers_ws_TE %>%
  select(ID_testdate, age_continuous, years_education, lang_exp_eng, lang_exp_fre, lang_exp_other) %>%
  pivot_longer(-c(ID_testdate), names_to = "info", values_to = "value") %>%
  group_by(info) %>%
  summarise(n_subject = n_distinct(ID_testdate),
            mean = mean(value, na.rm=T),
            sd = sd(value, na.rm=T),
            min = min(value, na.rm=T),
            max = max(value, na.rm=T))
```

## who filled out the CDIs
```{r}
keepers_ws_TE %>% 
  # combine grandmother & other family member
  mutate(cdi_filled_by = replace(cdi_filled_by, cdi_filled_by == "Grandmother", "Other family member")) %>% 
  # count number of respondents
  count(cdi_filled_by) %>%
  mutate(percentage = n/sum(n)*100) # calculate percentages
```

## average # of vocabulary
```{r}
vocabulary_average <- keepers_ws_TE %>%
  select(ID_testdate, word_vocab, TotalNW_Dom, TotalNW_NonDom, concept_vocab,
         number_of_te, Dom_unique_words, NonDom_unique_words) %>%
  pivot_longer(-ID_testdate, names_to = "vocabulary", values_to = "number") %>%
  group_by(vocabulary) %>%
  summarise(mean = mean(number, na.rm = T), 
            sd = sd(number, na.rm = T),
            min = min(number, na.rm = T),
            max = max(number, na.rm = T)) %>%
  mutate_if(is.numeric, round, digits = 2)

## paired-sample t-test to compare DOM and NONDOM
t.test(keepers_ws_TE$TotalNW_Dom, keepers_ws_TE$TotalNW_NonDom, 
       paired = TRUE) 

## paired-sample t-test to compare DI and NI
t.test(keepers_ws_TE$Dom_unique_words, keepers_ws_TE$NonDom_unique_words, 
       paired = TRUE) 
```

## average balance and comparison with balance by input
```{r}
# Average BALANCE score (defined by vocabulary balance)
keepers_ws_TE %>%
  summarise(mean = mean(balance_vocab, na.rm = T),
            sd = sd(balance_vocab, na.rm = T),
            min = min(balance_vocab, na.rm = T),
            max = max(balance_vocab, na.rm = T))

# grouped by language-dominance
keepers_ws_TE %>%
  group_by(LangDom_vocab) %>%
  summarise(mean = mean(balance_vocab, na.rm = T),
            sd = sd(balance_vocab, na.rm = T),
            min = min(balance_vocab, na.rm = T),
            max = max(balance_vocab, na.rm = T))

t.test(balance_vocab ~ LangDom_vocab, keepers_ws_TE) # t-test to compare the two language groups

# Comparsion with balance by input (which is defined with reference to the lang_dom defined by vocab size)
keepers_ws_TE %>%
  mutate(consistent_LangDom = if_else(lang_dom != LangDom_vocab, 0, 1)) %>% # 1= consistent, 0 = inconsistent
  summarise(n = n(),
            n_consistent = sum(consistent_LangDom),
            percentage_consistent = n_consistent/n*100,
            n_inconsistent = n - n_consistent,
            percentage_inconsistent = n_inconsistent/n*100)

keepers_ws_TE_withoutInconsistent <- keepers_ws_TE %>%
  mutate(consistent_LangDom = if_else(lang_dom != LangDom_vocab, 0, 1)) %>% # 1= consistent, 0 = inconsistent
  filter(consistent_LangDom == 1)

## Correlation between balance by vocabulary and balance by input
#cor.test(keepers_ws_TE$balance_vocab, keepers_ws_TE$balance_input, method = "pearson")

### Correlation between balance by vocabulary and raw language exposure to the non-dominant language
cor.test(keepers_ws_TE$balance_vocab, keepers_ws_TE$LangNonDom_input, method = "pearson")

balance_correlation <- plot(keepers_ws_TE$balance_vocab, keepers_ws_TE$LangNonDom_input,
                            xlab = "Vocabulary balance (= N/W)",
                            ylab = "Raw exposure to the non-dominant language",
                            cex.lab=1.0) +
  #lines(lowess(keepers_ws_TE$balance_vocab, keepers_ws_TE$LangNonDom_input), col="blue") # lowess line (x,y)
  abline(lm(keepers_ws_TE$LangNonDom_input~keepers_ws_TE$balance_vocab), col="red") # regression line (y~x)

balance_correlation <- keepers_ws_TE %>%
  ggplot(aes(x = balance_vocab, y = LangNonDom_input)) +
  stat_smooth(method = lm, se = F, color = "black") +
  geom_point(shape = 1) + 
  theme_light() + 
  labs(x = "Vocabulary BALANCE (= NONDOM/WORD)", 
       y = "Raw exposure to the non-dominant language (%)") +
  theme(text = element_text(size=12))

ggsave("Output/plot_BALANCE_correlation.png", balance_correlation,
       width = 6, height = 5)

```


# Percentile information from WordbankR
```{r}

## English Word&Sentence: Vocabulary size at 90% percentile
english_ws_percentile <-  
  fit_vocab_quantiles(get_administration_data("English (American)", "WS"), 
                      production,
                      quantile = "standard") %>%
  # keep only the 90% percentile
  filter(quantile == 0.90) %>%
  rename(EngWS_90percentile = production)

## Canadian French Word&Sentence: Vocabulary size at 90% percentile
french_ws_percentile <-  
  fit_vocab_quantiles(get_administration_data("French (Quebecois)", "WS"), 
                      production,
                      quantile = "standard") %>%
  # keep only the 90% percentile
  filter(quantile == 0.90) %>%
  rename(FrWS_90percentile = production)

## Merge the two percentile dataframe
ws_percentile <- merge(english_ws_percentile, french_ws_percentile, 
                       by = "age") %>%
  # compute estimate total vocab by adding the two languages
  mutate(Average_90percentile = (EngWS_90percentile + FrWS_90percentile)/2) %>%
  # keep only necessary columns
  select(age, EngWS_90percentile, FrWS_90percentile, Average_90percentile) %>%
  # rename age to prepare for the next step (i.e., merging with keepers_ws_TE
  rename(age_months_percentile = age)

write.csv(ws_percentile, "Output/ws_percentile.csv", row.names = F)
```


# CDI percentile for the hypothetical child Jamie
```{r}
english_ws_percentile_18 <-  
  fit_vocab_quantiles(get_administration_data("English (American)", "WS"), 
                      production,
                      quantile = "standard") %>%
  filter(age == 18) %>%
  rename(English = production) 

french_ws_percentile_18 <-  
  fit_vocab_quantiles(get_administration_data("French (Quebecois)", "WS"), 
                      production,
                      quantile = "standard") %>%
  filter(age == 18) %>%
  rename(French = production) 

ws_percentile_18 <- merge(english_ws_percentile_18, french_ws_percentile_18, 
                       by = c("form", "age","quantile")) %>%
  # compute estimate total vocab by adding the two languages
  mutate(Average_90percentile = (English + French)/2) %>%
  select(-c(language.x, language.y))

```

# Adding percentile info to keepers_ws_TE dataframe & 
## Creating a new propTE based on the percentile
```{r}
# merge the dataframes
keepers_ws_TE <- keepers_ws_TE %>%
  # as the upper age limit of the CDI-WS is 30m, create a new age variable which replaces all >30m as 30m for merging with the percentile dataframe
  # this means the percentile should be the same for all >30m
  mutate(age_months_percentile = ifelse(age_months_binned > 30, 30, age_months_binned)) %>%
  # merge with the percentile data frame
  left_join(ws_percentile, by = "age_months_percentile") 
```


# Correlation among variables in the observed data
```{r}
Correlation <- keepers_ws_TE %>%
  select(age_continuous, Average_90percentile, balance_vocab, word_vocab, TotalNW_Dom, TotalNW_NonDom,
         number_of_te, Dom_unique_words, NonDom_unique_words, concept_vocab) %>%
  # rename variables
  rename(Age_month = age_continuous,
         LEARNABLE = Average_90percentile,
         BALANCE = balance_vocab, 
         WORD = word_vocab,
         DOM = TotalNW_Dom, 
         NONDOM = TotalNW_NonDom, 
         TE = number_of_te, 
         "DOM-SINGLET" = Dom_unique_words, 
         "NONDOM-SINGLET" = NonDom_unique_words, 
         CONCEPT = concept_vocab)

# Correlation with corrections for multiple comparisons
library(psych)
TE_corr <- corr.test(Correlation, use = "pairwise", method = "pearson", adjust = "BY")

write.csv(round(TE_corr$r, 3), file="Output/ObservedTE_correlations_r.csv")  ## coefficients
write.csv(round(TE_corr$p, 3), file="Output/ObservedTE_correlations_p.csv")  ## p-values


library(Hmisc)
mycor <- rcorr(as.matrix(Correlation), type="pearson")
mycor$r
round(mycor$P, 3)
round(p.adjust(mycor$P, "BY"))

library(corrplot)
corrplot(mycor)
write.csv(mycor, "Output/ObservedTE_correlations.csv", col.names=NA)
```


# Simulation 1: simulating infants of the same developmental level with different word vocabularies and balances of exposure (LEARNABLE = 600)
```{r}
## generate the simulation data
simulation1 <- function (LEARNABLE) {
  DOM_seq <- seq(100, LEARNABLE, by = 100) # sequence of D from 100 to V at an interval of 100
  NONDOM_seq <- seq(0, LEARNABLE, by = 10) # sequence of N from 0 to V at an interval of 10
  DOM <- rep (DOM_seq, length(NONDOM_seq))
  NONDOM <- rep(NONDOM_seq, length(DOM_seq))
  data_simulation1 <<- data.frame(LEARNABLE, DOM, NONDOM)
}

simulation1(600) # setting V = 600

## clean the simulation data
data_simulation1 <- data_simulation1 %>%
  arrange(DOM, NONDOM) %>%
  filter(NONDOM <= DOM) %>%
  # Get word vocabulary W
  mutate(WORD = DOM + NONDOM) %>%
  # Create other variables
  mutate(BALANCE = NONDOM/(DOM+NONDOM), 
         TE = (DOM*NONDOM)/LEARNABLE) %>% # If DOM and NONDOM are independent, TE = DOM*NONDOM/LEARNABLE
  # Round to keep whole numbers only
  mutate_at(vars(-BALANCE), round, 0) %>%
  # bin B into 5 groups
  mutate(bin_BALANCE = cut(BALANCE, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), include.lowest=T,
                     labels = c("0.1", "0.2", "0.3", "0.4", "0.5"))) %>%
  arrange(DOM, NONDOM, bin_BALANCE)

## visualizing Simulation 1
simulation1 <- data_simulation1 %>%
  # convert to long data
  pivot_longer(c(WORD, DOM, NONDOM), names_to = "vocab_type", values_to = "number") %>%
  mutate(vocab_type = factor(vocab_type, levels=c("WORD", "DOM", "NONDOM"))) %>%
  mutate(vocab_type = recode(vocab_type, 
                             DOM = "Panel 1B: Dominant vocabulary",
                             NONDOM = "Panel 1C: Non-dominant vocabulary",
                             WORD = "Panel 1A: Total vocabulary")) %>%
  ggplot(aes(x = number, y = TE, color = bin_BALANCE, linetype = bin_BALANCE)) +
  stat_smooth(method = lm, se = F) +
#  scale_color_brewer(palette="Accent")+
  scale_color_manual(values=c("#762a83", "#abd9e9", "#a6d96a", "#fdae61", "#d53e4f")) + 
  scale_linetype_manual(values=c("solid", "solid", "solid", "solid", "solid")) +
  theme_minimal() + 
  facet_grid(. ~ vocab_type) +
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "bottom",
        plot.title = element_text(face = 'bold', size = 18, hjust = 0, vjust = 2),
        plot.title.position = "plot",
        text = element_text(size=14),
        panel.spacing.x = unit(2, "lines"),
        panel.background = element_rect(fill = NA, color = "black")) +
  ylim(0, 600) +
  xlim(0, 1000) +
  labs(x = "Number of words", 
       y = "Total number of translation equivalents (TE)", 
       color = "Vocabulary balance (BALANCE)",
       linetype = "Vocabulary balance (BALANCE)",
       title = "Simulated data") +
  guides(color = guide_legend(reverse = T, nrow = 1),
         linetype = guide_legend(reverse = T, nrow = 1)) 

ggsave("Output/plot_Simulation1.png", simulation1,
       width = 10, height = 5)
```

## Observed data: Testing Simulation 1
```{r}
Observed_plot_inputs_TE <- keepers_ws_TE %>% 
  select(baby_id, ID_testdate, age_days, number_of_te, word_vocab, TotalNW_Dom, TotalNW_NonDom, balance_vocab) %>%
  pivot_longer(c(word_vocab, TotalNW_Dom, TotalNW_NonDom), names_to = "vocab_type", values_to = "number") %>%
  mutate(vocab_type = factor(vocab_type, levels=c("word_vocab", "TotalNW_Dom", "TotalNW_NonDom"))) %>%
  mutate(vocab_type = recode(vocab_type, 
                             TotalNW_Dom = "Panel 2B: Dominant vocabulary", 
                             TotalNW_NonDom = "Panel 2C: Non-dominant vocabulary",
                             word_vocab = "Panel 2A: Total vocabulary")) %>%
  mutate(bin_BALANCE = cut(balance_vocab, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), include.lowest=T,
                     labels = c("0.1", "0.2", "0.3", "0.4", "0.5"))) %>%
  ggplot(aes(x = number, y = number_of_te, color = bin_BALANCE, linetype = bin_BALANCE)) +
  #geom_point() +
  stat_smooth(method = lm, se = F) +
  scale_color_manual(values=c("#762a83", "#abd9e9", "#a6d96a", "#fdae61", "#d53e4f")) + 
  scale_linetype_manual(values=c("solid", "solid", "solid", "solid", "solid")) +
  theme_minimal() + 
  facet_grid(. ~ vocab_type) +
  theme(axis.text.x = element_text(angle = 90),
        plot.title = element_text(face = 'bold', size = 18, hjust = 0, vjust = 2),
        plot.title.position = "plot",
        text = element_text(size=14),
        legend.position = "bottom",
        panel.spacing.x = unit(2, "lines"),
        panel.background = element_rect(fill = NA, color = "black")) +
  ylim(0, 600) +
  xlim(0, 1000) +
  labs(x = "Number of words", 
       y = "Total number of translation equivalents (TE)", 
       color = "Vocabulary balance (BALANCE)",
       linetype = "Vocabulary balance (BALANCE)",
       title = "Observed data") +
  guides(colour = guide_legend(reverse = T, nrow = 1),
         linetype = guide_legend(reverse = T, nrow = 1))

  
plot_Simulation1plusData <- ggarrange(simulation1, 
                                      Observed_plot_inputs_TE,
                                      nrow = 2,
                                      common.legend = TRUE, legend = "bottom")

ggsave("Output/BU_plot_Simulation1plusData.png", plot_Simulation1plusData,
       width = 10, height = 10)


# testing Prediction 1a: To compare TE among the 5 BALANCE groups 
## correlation between TE and BALANCE
cor.test(Correlation$TE, Correlation$BALANCE, method = "pearson")

## one-way ANOVA
aov_TE_binBALANCE <- keepers_ws_TE %>% 
  mutate(bin_BALANCE = cut(balance_vocab, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), include.lowest=T,
                     labels = c("0.1", "0.2", "0.3", "0.4", "0.5"))) %>%
  aov(number_of_te ~ bin_BALANCE, data = .)

summary(aov_TE_binBALANCE)

check <- keepers_ws_TE %>% 
  mutate(bin_BALANCE = cut(balance_vocab, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), include.lowest=T,
                     labels = c("0.1", "0.2", "0.3", "0.4", "0.5"))) 

pairwise.t.test(check$number_of_te, check$bin_BALANCE,
                p.adjust.method = "BH")

mean_TE_binBALANCE <- keepers_ws_TE %>% 
  mutate(bin_BALANCE = cut(balance_vocab, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), include.lowest=T,
                     labels = c("0.1", "0.2", "0.3", "0.4", "0.5"))) %>%
  group_by(bin_BALANCE) %>%
  summarise(n = n(),
            mean_TE = mean(number_of_te, na.rm = T),
            sd_TE = sd(number_of_te, na.rm = T),
            min_TE = min(number_of_te, na.rm = T),
            max_TE = max(number_of_te, na.rm = T)) %>%
  mutate(across(!bin_BALANCE & !sd_TE, ~ round(.,0))) %>%
  mutate_at(vars(sd_TE), ~ round(.,2))
  
# testing Prediction 1b: To compare TE and WORD or DOM
cor.test(Correlation$TE, Correlation$WORD, method = "pearson")
cor.test(Correlation$TE, Correlation$DOM, method = "pearson")

# testing Prediction 1c: To compare TE and NONDOM
cor.test(Correlation$TE, Correlation$NONDOM, method = "pearson")
```



# Simulation 2: simulating patterns of translation equivalent learning as a function of different developmental timepoints (LEARNABLE = 300, 450, or 600)
```{r}
## generate the simulation data
LEARNABLE <- seq(300, 600, by = 150) # set LEARNABLE to 300, 450, and 600

simulation2 <- function (LEARNABLE) {
  DOM_seq <- seq(100, LEARNABLE, by = 100) # sequence of D from 100 to V at an interval of 100
  NONDOM_seq <- seq(0, LEARNABLE, by = 25) # sequence of N from 0 to V at an interval of 25
  DOM <- rep(DOM_seq, length(NONDOM_seq))
  NONDOM <- rep(NONDOM_seq, length(DOM_seq))
  data_simulation2 <<- data.frame(LEARNABLE, DOM, NONDOM)
}

data_simulation2 <- do.call(rbind, lapply(LEARNABLE, simulation2)) %>%
  arrange(LEARNABLE, DOM, NONDOM) %>%
  filter(NONDOM <= DOM) %>%
  # Create other variables
  mutate(WORD = DOM + NONDOM, # word vocabulary 
         BALANCE = NONDOM/(DOM+NONDOM), # balance,
         TE = (DOM*NONDOM)/LEARNABLE, # If D and N are independent, TE = DN/V
         DOM_SINGLET = DOM-TE, # Unique words in dominant language
         NONDOM_SINGLET = NONDOM-TE) %>% # Unique words in non-dominant language
  # Round to keep whole numbers only
  mutate_at(vars(-BALANCE), round, 0) %>%
  # bin B into 3 groups
  mutate(bin_BALANCE = case_when(BALANCE >= 0.36 ~ "Most Balanced",
                                 BALANCE <= 0.20 ~ "Least Balanced",
                                 TRUE ~ "Medium Balanced")) %>%
  mutate(bin_BALANCE  = fct_rev(bin_BALANCE)) %>%
  arrange(DOM, NONDOM, bin_BALANCE)

## visualizing Simulation 2
simulation2 <- data_simulation2 %>%
  pivot_longer(-c(LEARNABLE, WORD, BALANCE, bin_BALANCE), names_to = "vocab_type", values_to = "number") %>%
  filter(vocab_type %notin% c("DOM", "NONDOM")) %>%
  mutate(vocab_type = factor(vocab_type, levels=c("NONDOM_SINGLET", "DOM_SINGLET", "TE"))) %>%
  mutate(LEARNABLE_label = case_when(LEARNABLE == 600 ~ "LEARNABLE = 600",
                                     LEARNABLE == 450 ~ "LEARNABLE = 450",
                                     LEARNABLE == 300 ~ "LEARNABLE = 300")) %>%
  group_by(LEARNABLE, LEARNABLE_label, bin_BALANCE, vocab_type) %>%
  summarise(n_words = mean(number)) %>%
  ggplot(aes(x = bin_BALANCE, y = n_words, fill = vocab_type)) + 
  geom_bar(position="stack", stat="identity") +
  facet_grid(. ~ LEARNABLE_label) +
  scale_fill_manual(values = c("#bdbdbd", "#737373", "#252525")) +
  scale_x_discrete(labels = function(bin_BALANCE) str_wrap(bin_BALANCE, width = 10)) + 
  ylim(0, 600) +
  theme_minimal() +
  theme(legend.position = "bottom",
        plot.title = element_text(face = 'bold', size = 18, hjust = 0, vjust = 2),
        plot.title.position = "plot",
        text = element_text(size=14),
        panel.spacing.x = unit(2, "lines")) + 
  guides(fill=guide_legend(nrow=3,byrow=TRUE)) + 
  labs(x = "\nVocabulary balance (BALANCE)", 
       y = "Number of concepts (CONCEPT)", 
       title = "Panel A: Simulated data")

ggsave("Output/plot_Simulation2.png", simulation2,
        width = 10, height = 5)
```

## Observed data: Testing Simulation 2
```{r}
plot_stacked_byLEARNABLE <- keepers_ws_TE %>%
  select(baby_id, age_months_percentile, balance_vocab, word_vocab, number_of_te, Dom_unique_words, NonDom_unique_words) %>%
  # Subset developmental level (i.e., Average_90percentile) by 3 levels 
  mutate(LEARNABLE_subset = case_when(age_months_percentile <= 22 ~ "18-22 months \n (LEARNABLE = 244.9 - 451.9)",
                                      age_months_percentile > 22 & age_months_percentile <= 27 ~ "23-27 months \n (LEARNABLE = 491.8 - 604.1)",
                                      TRUE ~ "28-33 months \n (LEARNABLE = 620.4 - 638.9)")) %>%
  mutate(BalSubset = case_when(balance_vocab > 0.35 ~ "Most Balanced",
                               balance_vocab <= 0.20 ~ "Least Balanced",
                               TRUE ~ "Medium Balanced")) %>%
  mutate(BalSubset = fct_rev(BalSubset)) %>%
  arrange(desc(number_of_te)) %>%
  mutate(TE = number_of_te) %>%
  pivot_longer(-c(baby_id, age_months_percentile, balance_vocab, LEARNABLE_subset, BalSubset, word_vocab, TE), 
               names_to = "Type", values_to = "NumberOfWords") %>%
  mutate(Type = factor(Type, levels=c("NonDom_unique_words", "Dom_unique_words", "number_of_te"))) %>%
  mutate(Type = recode(Type, 
                       NonDom_unique_words = "Singlet in non-dominant language (NONDOM-SINGLET)", 
                       Dom_unique_words = "Singlet in dominant language (DOM-SINGLET)", 
                       number_of_te = "Translation equivalent (TE)")) %>%
  group_by(LEARNABLE_subset, BalSubset, Type) %>%
  summarise(n_words = mean(NumberOfWords)) %>%
  ggplot(aes(x = BalSubset, y = n_words, fill = Type)) +
  geom_bar(position="stack", stat="identity") +
  facet_grid(. ~ LEARNABLE_subset) +
  scale_fill_manual(values = c("#bdbdbd", "#737373", "#252525")) +
  scale_x_discrete(labels = function(BalSubset) str_wrap(BalSubset, width = 10)) + 
  ylim(0, 600) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0) , 
        legend.position = "bottom",
        plot.title = element_text(face = 'bold', size = 18, hjust = 0, vjust = 2),
        plot.title.position = "plot",
        text = element_text(size=14),
        panel.spacing.x = unit(2, "lines")) + 
  guides(fill=guide_legend(nrow=3,byrow=TRUE)) + 
  labs(x = "\nVocabulary balance (BALANCE)", 
       y = "Number of concepts (CONCEPT)",
       title = "Panel B: Observed data") 


plot_Simulation2plusData <- ggarrange(simulation2 + rremove("legend"), 
                                      plot_stacked_byLEARNABLE,
                                      nrow = 2,
                                      heights=c(0.8,1))

ggsave("Output/plot_Simulation2plusData.png", plot_Simulation2plusData,
       width = 10, height = 10)

# testing Prediction 2a: To compare CONCEPT among the 3 LEARNABLE groups in the stacked bar chart
## correlation between age (in months) and CONCEPT
cor.test(Correlation$CONCEPT, Correlation$Age_month, method = "pearson")

## one-way ANOVE to compare CONCEPT among LEARNABLE
keepers_ws_TE_3LEARNABLEsubset <- keepers_ws_TE %>%
  # Subset developmental level (i.e., Average_90percentile) by 3 levels 
  mutate(LEARNABLE_subset = case_when(age_months_percentile <= 22 ~ "18-22 months (V = 244.9 - 451.9)",
                              age_months_percentile > 22 & age_months_percentile <= 27 ~ "23-27 months (V = 491.8 - 604.1)",
                              TRUE ~ "28-33 months (V = 620.4 - 638.9)"))

summary(aov(concept_vocab ~ LEARNABLE_subset, data = keepers_ws_TE_3LEARNABLEsubset))

mean_concept_LEARNABLEsubset <- keepers_ws_TE_3LEARNABLEsubset %>%
  group_by(LEARNABLE_subset) %>%
  summarise(mean_concept = mean(concept_vocab, na.rm = T),
            sd_concept = sd(concept_vocab, na.rm = T),
            min_concept = min(concept_vocab, na.rm = T),
            max_concept = max(concept_vocab, na.rm = T)) %>%
  mutate(across(is.numeric, ~ round(.,1)))

pairwise.t.test(keepers_ws_TE_3LEARNABLEsubset$concept_vocab, keepers_ws_TE_3LEARNABLEsubset$LEARNABLE_subset,
                p.adjust.method = "BH")

# testing Prediction 2b: To compare TE among the 3 LEARNABLE groups in the stacked bar chart
## one-way ANOVA to compare TE among LEARNABLE
stacked_byLEARNABLE_wide <- keepers_ws_TE %>%
  select(baby_id, age_months_percentile, balance_vocab, word_vocab, concept_vocab, 
         number_of_te, Dom_unique_words, NonDom_unique_words) %>%
  # Subset developmental level (i.e., Average_90percentile) by 3 levels 
  mutate(LEARNABLE_subset = case_when(age_months_percentile <= 22 ~ "18-22 months",
                              age_months_percentile > 22 & age_months_percentile <= 27 ~ "23-27 months",
                              TRUE ~ "28-33 months")) %>%
  mutate(BalSubset = case_when(balance_vocab > 0.35 ~ "Most Balanced",
                               balance_vocab <= 0.20 ~ "Least Balanced",
                               TRUE ~ "Medium Balanced")) 

TE_byLEARNABLE_anova <- aov(number_of_te ~ LEARNABLE_subset, data = stacked_byLEARNABLE_wide)
summary(TE_byLEARNABLE_anova)

pairwise.t.test(stacked_byLEARNABLE_wide$number_of_te, stacked_byLEARNABLE_wide$LEARNABLE_subset,
                 p.adjust.method = "BH")

mean_TE_LEARNABLEsubset <- stacked_byLEARNABLE_wide %>%  
  group_by(LEARNABLE_subset) %>%
  summarise(mean_TE = mean(number_of_te, na.rm=T), 
            sd_TE = sd(number_of_te, na.rm=T),
            min_TE = min(number_of_te, na.rm=T),
            max_TE = max(number_of_te, na.rm=T)) %>%
  mutate(across(is.numeric, ~ round(.,1)))

## correlation between age (in months) and TE
cor.test(Correlation$TE, Correlation$Age_month, method = "pearson")


# testing Prediction 2c: One-way Anova to compare DOM-SINGLET among the 3 LEARNABLE groups in the stacked bar chart
## correlation between DOM-SINGLET and age (in month)
cor.test(Correlation$"DOM-SINGLET", Correlation$Age_month, method = "pearson")

## correlation between DOM-SINGLET and BALANCE
cor.test(Correlation$"DOM-SINGLET", Correlation$BALANCE, method = "pearson")

## one-way ANOVA to compare DOM-SINGLET among LEARNABLE
DOMSINGLET_byBALANCE_anova <- aov(Dom_unique_words ~ BalSubset, data = stacked_byLEARNABLE_wide)
summary(DOMSINGLET_byBALANCE_anova)

pairwise.t.test(stacked_byLEARNABLE_wide$Dom_unique_words, stacked_byLEARNABLE_wide$BalSubset,
                 p.adjust.method = "BH")

mean_DOMSINGLET_LEARNABLEsubset <- stacked_byLEARNABLE_wide %>%  
  group_by(BalSubset) %>%
  summarise(mean_DOM_SINGLET = mean(Dom_unique_words, na.rm=T), 
            sd_DOM_SINGLET = sd(Dom_unique_words, na.rm=T),
            min_DOM_SINGLET = min(Dom_unique_words, na.rm=T),
            max_DOM_SINGLET = max(Dom_unique_words, na.rm=T)) %>%
  mutate(across(is.numeric, ~ round(.,1)))


# testing Prediction 2d: One-way Anova to compare NONDOM-SINGLET among the 3 LEARNABLE groups in the stacked barchart
## correlation between NONDOM-SINGLET and age (in month)
cor.test(Correlation$"NONDOM-SINGLET", Correlation$Age_month, method = "pearson")

## correlation between NONDOM-SINGLET and BALANCE
cor.test(Correlation$"NONDOM-SINGLET", Correlation$BALANCE, method = "pearson")

## one-way ANOVA to compare NONDOM-SINGLET among LEARNABLE
NONDOMSINGLET_byBALANCE_anova <- aov(NonDom_unique_words ~ BalSubset, data = stacked_byLEARNABLE_wide)
summary(NONDOMSINGLET_byBALANCE_anova)

pairwise.t.test(stacked_byLEARNABLE_wide$NonDom_unique_words, stacked_byLEARNABLE_wide$BalSubset,
                 p.adjust.method = "BH")

mean_NONDOMSINGLET_LEARNABLEsubset <-stacked_byLEARNABLE_wide %>% 
  group_by(BalSubset) %>%
  summarise(mean_NONDOM_SINGLET = mean(NonDom_unique_words, na.rm=T), 
            sd_NONDOM_SINGLET = sd(NonDom_unique_words, na.rm=T),
            min_NONDOM_SINGLET = min(NonDom_unique_words, na.rm=T),
            max_NONDOM_SINGLET = max(NonDom_unique_words, na.rm=T)) %>%
  mutate(across(is.numeric, ~ round(.,1)))
```


# Simulation 3: Non-independence of dominant-language (DOM) and non-dominant language words (NONDOM)
```{r}
## generate the simulation data
LEARNABLE <- seq(150, 600, by = 150) # set LEARNABLE to 300, 450, and 600

simulation3 <- function (LEARNABLE) {
  DOM_seq = seq(100, LEARNABLE, by = 100) # sequence of DOM from 100 to LEARNABLE at an interval of 100
  NONDOM_seq = seq(0, LEARNABLE, by = 25) # sequence of NONDOM from 0 to LEARNABLE at an interval of 25
  DOM <- rep(DOM_seq, length(NONDOM_seq))
  NONDOM <- rep(NONDOM_seq, length(DOM_seq))
  data_simulation3 <<- data.frame(LEARNABLE, DOM, NONDOM)
}

data_simulation3 <- do.call(rbind, lapply(LEARNABLE, simulation3)) %>%
  arrange(LEARNABLE, DOM, NONDOM) %>%
  filter(NONDOM <= DOM) %>% # eliminate cases when NONDOM > DOM
  mutate(WORD = DOM+NONDOM, # word vocabulary
         BALANCE = NONDOM/(DOM+NONDOM), # balance 
         TE = DOM*NONDOM/LEARNABLE) %>% # If DOM and NONDOM are independent, TE = DOM*NONDOM/LEARNABLE
  mutate_at(vars(-BALANCE), round, 0) %>%
  mutate(bin_BALANCE = cut(BALANCE, breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5), include.lowest=T,
                     labels = c("0.1", "0.2", "0.3", "0.4", "0.5"))) %>%
  arrange(bin_BALANCE)


## Simulation3: TE = BIAS * (DOM*NONDOM/LEARNABLE)
### defining the bias parameter I
BIAS1 = 1.5 # BIAS > 1, TEs are MORE easily learned than unique words
BIAS2 = 0.5 # BIAS < 1, TEs are LESS easily learned than unique words

### visualizing the simulation
simulation3 <- data_simulation3 %>%
  mutate(TE_easier = BIAS1*(DOM*NONDOM/LEARNABLE), # calculate where BIAS > 1
         TE_harder = BIAS2*(DOM*NONDOM/LEARNABLE)) %>% # calculate where BIAS < 1
  # round to keep whole numbers only
  mutate_at(vars(-c(BALANCE, bin_BALANCE)), round, 0) %>%
  # prepare dataset to plot
  pivot_longer(c(TE, TE_harder, TE_easier), 
               names_to = "Model", values_to = "number_TE") %>%
  mutate(Model = factor(Model, levels=c("TE_harder", "TE", "TE_easier"))) %>%
  mutate(Model = recode(Model, 
                        TE = "BIAS = 1 (Neutral Account)",
                        TE_easier = "BIAS = 1.5 (Preference Account)",
                        TE_harder = "BIAS = 0.5 (Avoidance Account)")) %>%
  # ggplot
  ggplot(aes(x = WORD, y = number_TE, color = bin_BALANCE, linetype = bin_BALANCE)) +
  stat_smooth(method = lm, se = F) +
  scale_color_manual(values=c("#bdbdbd", "#737373", "#525252", "#252525", "#000000")) + 
  scale_linetype_manual(values=c("longdash", "dotdash", "dashed", "twodash", "solid")) + 
  theme_minimal() + 
  facet_grid(. ~ Model) +
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "bottom") +
  theme(plot.title = element_text(hjust = 0.5),
        text = element_text(size=12),
        panel.spacing.x = unit(2.5, "lines"),
        panel.background = element_rect(fill = NA, color = "black")) +
  labs(x = "Total word vocabulary (WORD)", 
       y = "Total number of translation equivalents (TE)", 
       color = "Vocabulary balance (BALANCE)",
       linetype = "Vocabulary balance (BALANCE)") +
  guides(colour = guide_legend(reverse = T, nrow = 1),
         linetype = guide_legend(reverse = T, nrow = 1))

ggsave("Output/plot_Simulation3.png", simulation3, 
       width = 10, height = 5)
```

# Testing Simulation 3: Figures 7 & 8
## Theoretical distribution of TEs (Simulation): D * N / V (TotalNW_Dom * TotalNW_NonDom / Average_90percentile) 
```{r}
# testing Prediction 3: linear regression model comparing simulated and observed TE
## creating predicted TEs for simulation
keepers_ws_TE_simulation <- keepers_ws_TE %>%
  mutate(Predicted_TE_percentile = TotalNW_Dom * TotalNW_NonDom/Average_90percentile) %>%
  select(baby_id, ID_testdate, Predicted_TE_percentile, number_of_te, Average_90percentile, TotalNW_Dom, TotalNW_NonDom, word_vocab, age_months_binned)

##linear regression model
Percentile_model <- keepers_ws_TE_simulation %>%
  lm(number_of_te*(Average_90percentile) ~ 0 + TotalNW_Dom:TotalNW_NonDom, 
     data = .)
summary(Percentile_model)

## ploting Y = observed rawTE  & X = Predicted rawTE 
plot_TEprediction_percentile <- keepers_ws_TE_simulation %>%
  ggplot(aes(x = Predicted_TE_percentile, y = number_of_te)) +
  labs(x = "Number of TEs predicted by the Bilingual Vocabulary Model", y = "Observed total number of TEs") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm", se = F) +
  geom_point(shape = 23) +
  geom_abline(linetype = 2, alpha = 0.5) +
  xlim(-10,450) +
  ylim(-10,450) +
  theme_minimal()

ggsave(here("Output/plot_Simulation3_data.png"), plot_TEprediction_percentile,
       height=5, width=5)

## identifying outliers
CheckOutliers_model <- keepers_ws_TE_simulation %>%
  lm(number_of_te*(Average_90percentile) ~ 0 + TotalNW_Dom:TotalNW_NonDom,
     data = .)
summary(CheckOutliers_model)

keepers_ws_TE_simulation$CD <- cooks.distance(CheckOutliers_model)

TEprediction_percentile_CD <- keepers_ws_TE_simulation %>%
  ggplot(aes(x = Predicted_TE_percentile, y = number_of_te)) +
  labs(x = "Number of TEs predicted by the Bilingual Vocabulary Model", y = "Observed total number of TEs") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm", se = T) +
  geom_point(size=3, aes(color=CD)) +
  geom_text(aes(label=ID_testdate)) +
  geom_abline(linetype = 2, alpha = 0.5) +
  theme_minimal()

qplot_CD <- qplot(keepers_ws_TE_simulation$CD)

grid.arrange(TEprediction_percentile_CD, qplot_CD, ncol = 2)

keepers_ws_TE_simulation_removeCD <- filter(keepers_ws_TE_simulation, CD < 0.4)

TEprediction_percentile_removeCD <- keepers_ws_TE_simulation_removeCD %>%
  ggplot(aes(x = Predicted_TE_percentile, y = number_of_te)) +
  labs(x = "Number of TEs predicted by the Bilingual Vocabulary Model", y = "Observed total number of TEs") +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_smooth(method = "lm", se = T) +
  geom_point(size=3) +
  geom_text(aes(label=ID_testdate)) +
  geom_abline(linetype = 2, alpha = 0.5) +
  theme_minimal()

grid.arrange(TEprediction_percentile_removeCD, TEprediction_percentile_CD, ncol = 2)

Percentile_model_removeCD <- keepers_ws_TE_simulation_removeCD %>%
  lm(number_of_te*(Average_90percentile) ~ 0 + TotalNW_Dom:TotalNW_NonDom, 
     data = .)

summary(Percentile_model_removeCD)


# splitting the model for children with WORD < 300 and those with WORD > 300
## linear regression model for those with WORD > 300
Percentile_model_MoreThan300W <- keepers_ws_TE_simulation %>%
  filter(word_vocab > 300) %>%
  lm(number_of_te*(Average_90percentile) ~ 0 + TotalNW_Dom:TotalNW_NonDom, 
     data = .)
summary(Percentile_model_NoOutliers_MoreThan300W)

## linear regression model for those with WORD < 300
Percentile_model_LessThan300W <- keepers_ws_TE_simulation %>%
  filter(word_vocab < 300) %>%
  lm(number_of_te*(Average_90percentile) ~ 0 + TotalNW_Dom:TotalNW_NonDom, 
     data = .)
summary(Percentile_model_NoOutliers_LessThan300W)

## plotting the two separate models (those with WORD < 300 + those with WORD > 300)
TEprediction_percentile_facet_lessW300 <- keepers_ws_TE_simulation %>%
  mutate(W_group = if_else(word_vocab < 300, "Less than 300 total vocabulary", "More than 300 total vocabulary")) %>%
  filter(W_group == "Less than 300 total vocabulary") %>%
  ggplot(aes(x = Predicted_TE_percentile, y = number_of_te)) +
  labs(x = "Number of TEs predicted by the Bilingual Vocabulary Model", y = "Observed total number of TEs",
       title = "Less than 300 total vocabulary") +
  xlim(0,80) +
  geom_smooth(method = "lm", se = F) +
  geom_point(shape = 23) +
  geom_abline(linetype = 2, alpha = 0.5) +
  theme_minimal(base_size = 18) +
  theme(plot.title = element_text(hjust = 0.5))

TEprediction_percentile_facet_moreW300 <- keepers_ws_TE_simulation %>%
  mutate(W_group = if_else(word_vocab < 300, "Less than 300 total vocabulary", "More than 300 total vocabulary")) %>%
  filter(W_group == "More than 300 total vocabulary") %>%
  ggplot(aes(x = Predicted_TE_percentile, y = number_of_te)) +
  labs(x = "Number of TEs predicted by the Bilingual Vocabulary Model", y = "Observed total number of TEs",
       title = "More than 300 total vocabulary") +
  geom_smooth(method = "lm", se = F) +
  geom_point(shape = 23) +
  geom_abline(linetype = 2, alpha = 0.5) +
  theme_minimal(base_size = 18) +
  theme(plot.title = element_text(hjust = 0.5))

TEprediction_percentile_facetByW300 <- 
  ggarrange(TEprediction_percentile_facet_lessW300 + rremove("x.title"), 
            TEprediction_percentile_facet_moreW300 + rremove("xy.title"), 
            ncol = 2, nrow = 1) %>%
  annotate_figure(bottom = text_grob("Number of TEs predicted by the Bilingual Vocabulary Model", size= 18))

ggsave(here("Output/BU_plot_Simulation3_data_2WORDgroups.png"), TEprediction_percentile_facetByW300,
       height=5,width=10)
```
